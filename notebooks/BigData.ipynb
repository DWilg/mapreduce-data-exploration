{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1a5704f",
   "metadata": {},
   "source": [
    "# Big Data – Notebook demonstracyjny\n",
    "\n",
    "Ten notebook prezentuje kolejne kroki (\"kolejne krokki\") analizy oraz pipeline: MapReduce (lokalna symulacja), eksploracja, przygotowanie cech, model bazowy i ewaluacja. Sekcje zgodne z listą kroków w zadaniu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10a95b1",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "Wczytujemy przykładowy plik transakcji CSV z folderu `data/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2156b203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "DATA_PATH = os.path.join('..', 'data', 'transactions_sample.csv')\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print('Shape:', df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67603050",
   "metadata": {},
   "source": [
    "## 2. Validate Raw Schema\n",
    "Sprawdzamy obecność i typy kolumn oraz podstawowe reguły domenowe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f6038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_cols = {'transaction_id','user_id','amount','category','timestamp'}\n",
    "assert expected_cols.issubset(df.columns), f\"Brak kolumn: {expected_cols - set(df.columns)}\"\n",
    "assert df['amount'].dtype in (float, int, 'float64', 'int64', 'int32', 'float32'), 'Kolumna amount powinna być numeryczna'\n",
    "negatives = df[df['amount'] < 0]\n",
    "assert negatives.empty, f\"Wykryto ujemne kwoty: {len(negatives)}\"\n",
    "print('Walidacja surowych danych OK.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141a79dc",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning & Missing Values\n",
    "Analizujemy brakujące wartości i stosujemy proste uzupełnienia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eb5f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = df.isna().sum()\n",
    "print(null_counts)\n",
    "if 'category' in df.columns:\n",
    "    df['category'] = df['category'].fillna('UNKNOWN')\n",
    "if 'amount' in df.columns:\n",
    "    df['amount'] = df['amount'].fillna(df['amount'].median())\n",
    "print('Po czyszczeniu nulli:', df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748a7f2b",
   "metadata": {},
   "source": [
    "## 4. Exploratory Profiling\n",
    "Statystyki opisowe, rozkład i korelacje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4c1bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['amount'].describe())\n",
    "import seaborn as sns, matplotlib.pyplot as plt\n",
    "sns.set_theme()\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(df['amount'], bins=8, kde=True)\n",
    "plt.title('Rozkład amount')\n",
    "plt.show()\n",
    "\n",
    "num_cols = [c for c in df.columns if df[c].dtype != 'object']\n",
    "if num_cols:\n",
    "    corr = df[num_cols].corr()\n",
    "    print(corr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98382896",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "Tworzymy nowe cechy oraz kodowania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f2db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "if 'amount' in df.columns:\n",
    "    df['log_amount'] = np.log1p(df['amount'])\n",
    "if 'category' in df.columns:\n",
    "    categories = sorted(df['category'].unique())\n",
    "    cat_to_idx = {c:i for i,c in enumerate(categories)}\n",
    "    df['category_idx'] = df['category'].map(cat_to_idx)\n",
    "\n",
    "print(df[['amount','log_amount','category','category_idx']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a211b2",
   "metadata": {},
   "source": [
    "## 6. Train / Validation Split\n",
    "Dzielimy dane na zbiór treningowy i walidacyjny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df900880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "features = df[['user_id','category_idx']]\n",
    "labels = df['amount']\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "print(X_train.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687542b1",
   "metadata": {},
   "source": [
    "## 7. Baseline Model\n",
    "Tworzymy prosty model regresyjny (DummyRegressor) jako punkt odniesienia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa34d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "baseline = DummyRegressor(strategy='mean')\n",
    "baseline.fit(X_train, y_train)\n",
    "y_pred_base = baseline.predict(X_val)\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "print('Baseline MAE:', mean_absolute_error(y_val, y_pred_base))\n",
    "print('Baseline RMSE:', mean_squared_error(y_val, y_pred_base, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da46245",
   "metadata": {},
   "source": [
    "## 8. Model Improvement Loop\n",
    "Dodajemy bardziej zaawansowany model i prostą walidację krzyżową."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb327be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "cv_scores = cross_val_score(rf, features, labels, cv=3, scoring='neg_mean_absolute_error')\n",
    "print('CV MAE (neg):', cv_scores)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_val)\n",
    "print('RF MAE:', mean_absolute_error(y_val, y_pred_rf))\n",
    "print('RF RMSE:', mean_squared_error(y_val, y_pred_rf, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33238a5f",
   "metadata": {},
   "source": [
    "## 9. Evaluation Metrics & Error Analysis\n",
    "Analiza jakości i błędów modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c717a0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "errors = y_val - y_pred_rf\n",
    "print('Średni błąd:', errors.mean())\n",
    "print('Top 3 największe dodatnie błędy (niedoszacowanie):')\n",
    "print(errors.nlargest(3))\n",
    "print('Top 3 największe ujemne błędy (przeszacowanie):')\n",
    "print(errors.nsmallest(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb317443",
   "metadata": {},
   "source": [
    "## 10. Persist Model & Artifacts\n",
    "Zapisujemy wytrenowany model oraz metadane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbd5132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib, json\n",
    "os.makedirs('..\\\\output', exist_ok=True)\n",
    "model_path = '..\\\\output\\\\rf_model.joblib'\n",
    "joblib.dump(rf, model_path)\n",
    "meta = {\"features\": list(features.columns), \"target\": \"amount\"}\n",
    "with open('..\\\\output\\\\model_meta.json','w',encoding='utf-8') as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print('Zapisano model oraz metadane.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f582b1",
   "metadata": {},
   "source": [
    "## 11. Batch Inference Demo\n",
    "Ładujemy zapisany model i wykonujemy predykcję na próbce walidacyjnej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51938bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = joblib.load(model_path)\n",
    "val_sample = X_val.head(5)\n",
    "print('Wejście batch:', val_sample)\n",
    "print('Predykcje:', loaded.predict(val_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbf7351",
   "metadata": {},
   "source": [
    "## 12. Next Action Checklist\n",
    "- Analiza ważności cech\n",
    "- Monitoring driftu danych\n",
    "- Automatyzacja pipeline (CI/CD)\n",
    "- Parametryzacja hiperparametrów (Grid/Random/Bayesian Search)\n",
    "- Integracja z systemem produkcyjnym (API / batch)\n",
    "\n",
    "_Notebook zakończony._"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
